import time
import random
import tiktoken
import litellm
import openai


class LLMInvocation:
    def __init__(self, model: str):
        self.model = model

    def call_model(self, prompt: dict, max_tokens=4096, temperature=0.2):
        """
        Returns:
            tuple: A tuple containing the response generated by the LLM,
            the number of tokens used from the prompt, and the total number of tokens in the response.
        """
        if "system" not in prompt or "user" not in prompt:
            raise KeyError("The prompt dictionary must contain 'system' and 'user' keys.")

        if prompt["system"] == "":
            messages = [{"role": "user", "content": prompt["user"]}]
        else:
            messages = [
                {"role": "system", "content": prompt["system"]},
                {"role": "user", "content": prompt["user"]},
            ]

        if self.model == "deepseek-r1":
            # sample input
            completion_params = {
                "model": "sagemaker/endpoint-deepseek-r1-nashid",
                "messages": [{"role": "user", "content": "Are you better than GPT-4o for test generation and why?"}],
                "max_tokens": max_tokens,
                "stream": True,
                "temperature": temperature,
                "aws_region_name": "us-east-2"
            }
        else:
            completion_params = {
                "model": self.model,
                "messages": messages,
                "max_tokens": max_tokens,
                "stream": True,
                "temperature": temperature,
            }

        max_retries = 5
        base_delay = 2  # base delay in seconds

        for attempt in range(max_retries):
            try:
                response = litellm.completion(**completion_params)
                chunks = []
                try:
                    for chunk in response:
                        print(
                            chunk.choices[0].delta.content or "", end="", flush=True)
                        chunks.append(chunk)
                        time.sleep(0.01)
                except Exception as e:
                    print(f"Error during streaming: {e}")
                print("\n")
                model_response = litellm.stream_chunk_builder(
                    chunks, messages=messages)
                return (
                    model_response["choices"][0]["message"]["content"],
                    int(model_response["usage"]["prompt_tokens"]),
                    int(model_response["usage"]["completion_tokens"]),
                )
            except Exception as e:
                delay = base_delay * (2 ** attempt) + random.uniform(0, 1)
                print(f"Rate limit exceeded. "
                      f"Retrying in {delay:.2f} seconds... "
                      f"(Attempt {attempt + 1}/{max_retries})")
                time.sleep(delay)

        raise RuntimeError(
            "Max retries exceeded. Could not complete API call.")



class AzureOpenAIInvocation(LLMInvocation):
    def __init__(self, model: str, base_url: str, api_version: str, ak: str):
        super().__init__(model)
        self.client = openai.AzureOpenAI(
            azure_endpoint=base_url,
            api_version=api_version,
            api_key=ak,
        )
        
    def call_model(self, prompt: dict, max_tokens=4096, temperature=0.2):
        if "system" not in prompt or "user" not in prompt:
            raise KeyError(
                "The prompt dictionary must contain 'system' and 'user' keys.")

        if prompt["system"] == "":
            messages = [{"role": "user", "content": prompt["user"]}]
        else:
            messages = [
                {"role": "system", "content": prompt["system"]},
                {"role": "user", "content": prompt["user"]},
            ]

        completion_params = {
            "model": self.model,
            "messages": messages,
            "max_tokens": max_tokens,
            "stream": True,
            "temperature": temperature,
            "extra_headers": {"X-TT-LOGID": ""},
        }
        
        max_retries = 5
        base_delay = 2  # base delay in seconds

        for attempt in range(max_retries):
            try:
                response = self.client.chat.completions.create(**completion_params)
                chunks = []
                try:
                    for chunk in response:
                        print(
                            chunk.choices[0].delta.content or "", end="", flush=True)
                        chunks.append(chunk)
                        time.sleep(0.01)
                except Exception as e:
                    print(f"Error during streaming: {e}")
                print("\n")
                # fill complete content
                full_content = "".join(
                    chunk.choices[0].delta.content or "" 
                    for chunk in chunks
                )
                
                # calculate token count
                try:
                    encoding = tiktoken.encoding_for_model(self.model)
                    prompt_text = " ".join(msg["content"] for msg in messages)
                    prompt_tokens = len(encoding.encode(prompt_text))
                    completion_tokens = len(encoding.encode(full_content))
                except Exception:
                    # 如果tiktoken失败，回退到近似计算
                    prompt_tokens = int(len(" ".join(msg["content"] for msg in messages).split()) * 1.3)
                    completion_tokens = int(len(full_content.split()) * 1.3)
                
                return (full_content, prompt_tokens, completion_tokens)
            except Exception as e:
                delay = base_delay * (2 ** attempt) + random.uniform(0, 1)
                print(f"API Error: {e}")
                print(f"Rate limit exceeded. "
                      f"Retrying in {delay:.2f} seconds... "
                      f"(Attempt {attempt + 1}/{max_retries})")
                time.sleep(delay)

        raise RuntimeError(
            "Max retries exceeded. Could not complete API call.")
