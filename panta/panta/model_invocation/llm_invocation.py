import time
import random

import litellm


class LLMInvocation:
    def __init__(self, model: str):
        self.model = model

    def call_model(self, prompt: dict, max_tokens=4096, temperature=0.2):
        """
        Returns:
            tuple: A tuple containing the response generated by the LLM,
            the number of tokens used from the prompt, and the total number of tokens in the response.
        """
        if "system" not in prompt or "user" not in prompt:
            raise KeyError("The prompt dictionary must contain 'system' and 'user' keys.")

        if prompt["system"] == "":
            messages = [{"role": "user", "content": prompt["user"]}]
        else:
            messages = [
                {"role": "system", "content": prompt["system"]},
                {"role": "user", "content": prompt["user"]},
            ]

        if self.model == "deepseek-r1":
            # sample input
            completion_params = {
                "model": "sagemaker/endpoint-deepseek-r1-nashid",
                "messages": [{"role": "user", "content": "Are you better than GPT-4o for test generation and why?"}],
                "max_tokens": max_tokens,
                "stream": True,
                "temperature": temperature,
                "aws_region_name": "us-east-2"
            }
        else:
            completion_params = {
                "model": self.model,
                "messages": messages,
                "max_tokens": max_tokens,
                "stream": True,
                "temperature": temperature,
            }

        max_retries = 5
        base_delay = 2  # base delay in seconds

        for attempt in range(max_retries):
            try:
                response = litellm.completion(**completion_params)
                chunks = []
                try:
                    for chunk in response:
                        print(chunk.choices[0].delta.content or "", end="", flush=True)
                        chunks.append(chunk)
                        time.sleep(0.01)
                except Exception as e:
                    print(f"Error during streaming: {e}")
                print("\n")
                model_response = litellm.stream_chunk_builder(chunks, messages=messages)
                return (
                    model_response["choices"][0]["message"]["content"],
                    int(model_response["usage"]["prompt_tokens"]),
                    int(model_response["usage"]["completion_tokens"]),
                )
            except Exception as e:
                delay = base_delay * (2 ** attempt) + random.uniform(0, 1)
                print(f"Rate limit exceeded. "
                      f"Retrying in {delay:.2f} seconds... "
                      f"(Attempt {attempt + 1}/{max_retries})")
                time.sleep(delay)

        raise RuntimeError("Max retries exceeded. Could not complete API call.")
